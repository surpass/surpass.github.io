[{"categories":null,"contents":"  本文转载自：DevOps小站 官方网站，原文地址：https://www.easyolap.cn/creations/JenkinsIntergrationSonar/\n Jenkins 集成sonar 可以提供一个dashboard给项目成员和管理者，提供一个一目了然的项目分析情况，Sonar在代码分析是非常有用的工具。下面就具体说说如何进行集成。\n实践目标 * 安装和配置jenkins中sonar插件 * 配置sonar scanner * 修改scanner参数 * 执行构建\n一、环境准备 本文主要说明的是 Jenkins集成sonar的过程和配置以及使用，所以相关准备工作请参照相关文档。 以下是需要准备的环境   安装和配置sonar 安装和配置Jenkins 安装 sonar-scanner  ​\n二、Jenkins中Sonar插件的安装 插件方式安装。 * 在Jenkins中SonarQube可能通过“jenkins -》插件管理”安装;\n采用phi 文件安装方式： * 首先去这个url下载phi文件： http://updates.jenkins-ci.org/latest/sonar.hpi 这个是sonar最新插件，需要jenkins 2.6以上版本，可怜只好升级了一把jenkins。\n 准备好hpi文件，并升级jenkins，重启后，进入jenkins： jenkins -》插件管理-》高级  二、在Jenkins中配置 sonarqube server 进入jenkins-》系统管理-》系统配置 后，就可以看到配置sonarqube server相关内容：\n“Add SonarQube”后现如下界面内容： sonar的token如果忘记了可以sonerqube server的帐户-》安全-》用户中重新生成\n三、配置 SonarQube Scanner 进入jenkins-》系统管理-》全局工具配置 就可以看到SonarQube Scanner如下图：\n填写名称后去掉\u0026rdquo;自动安装选项\u0026rdquo;（此处可以用自动按装，由于网络问题我选择手动下载，解压后，在此配置\u0026rdquo;SONAR_RUNNER_HOME\u0026rdquo;）\n四、配置工程的scanner参数  进入工程的配置页面 构建配置组中选择“增加构建后步骤”   在Analysis properties中填入以下内容：\nsonar.host.url=http://10.10.171.220:9000/sonar sonar.projectKey=${projectName} sonar.projectName=${projectName} sonar.projectVersion=1.0.0 sonar.sourceEncoding=UTF-8 sonar.language=java sonar.sources=src sonar.java.binaries=target/classes  四、执行项目构建。 如果构建成功，可以到sonarqube server中能看到相关报告。 进入方式，因为jenkins已经与sonar集成了，所以在工程中有连接进入到sonarqube server,如下图：\n","permalink":"https://www.easyolap.cn/projects/creations/jenkinsintergrationsonar/","tags":["Nexus","Maven","Docker","Agile","DevOps"],"title":"Jenkins集成sonar使项目分析可视化 ！"},{"categories":null,"contents":"  本文转载自：DevOps小站 官方网站，原文地址：https://www.easyolap.cn/publications/nexus-002/\n 一、环境准备 安装好Nexus3环境，安装方法参见地址：https://www.easyolap.cn/publications/nexus-001-install-by-docker/。  ​\n二、添加Docker镜像库用户（可选） 方法:  以管理员登录\n 添加用户 (  三、创建本地镜像库 四、创建代理镜像库 五、创建分组镜像库 总结：   ","permalink":"https://www.easyolap.cn/publications/nexus-002/","tags":["Nexus","Kubernetes","Docker","Agile","DevOps"],"title":"Nexus build docker image Repository ！"},{"categories":null,"contents":"  本文转载自：DevOps小站 官方网站，原文地址：https://www.easyolap.cn/publications/nexus-001-install-by-docker/\n 一、环境准备 安装docker,本文使用的版本为docker Server Version: 17.11.0-ce 定义域名和购买域名，制作私有证书或购买，本文采用已有域名，申请免费证书实现。  ​\n二、下载镜像并启协服务 ATLASSIAN_HOME=/data/nexus/ docker run -d \u0026ndash;name nexus \u0026ndash;hostname nexus \u0026ndash;user root:root \u0026ndash;restart always -v $ATLASSIAN_HOME/work:/nexus-data -p 8081:8081 -p 8082:8082 -p 6000-6010:6001-6010 -e NEXUS_CONTEXT=nexus sonatype/nexus3:3.13.0\n三、修改容器内的配置文件 /opt/sonatype/nexus/etc/nexus-default.properties添加“application-port-ssl=8082” 和 修改nexus-args的值增加“${jetty.etc}/jetty-https.xml,”\n修改前\n \\# Jetty section application-port=8081 application-host=0.0.0.0 nexus-args=${jetty.etc}/jetty.xml,${jetty.etc}/jetty-http.xml,${jetty.etc}/jetty-requestlog.xml nexus-context-path=/${NEXUS_CONTEXT} \\# Nexus section nexus-edition=nexus-pro-edition nexus-features=\\ nexus-pro-feature nexus.clustered=false  修改后\n\\# Jetty section application-port=8081 application-port-ssl=8082 application-host=0.0.0.0 nexus-args=${jetty.etc}/jetty.xml,${jetty.etc}/jetty-http.xml,${jetty.etc}/jetty-https.xml,${jetty.etc}/jetty-requestlog.xml nexus-context-path=/${NEXUS_CONTEXT} \\# Nexus section nexus-edition=nexus-pro-edition nexus-features=\\ nexus-pro-feature nexus.clustered=false  四、启用https服务 修改容器内的${jetty.etc}/jetty-https.xml配置文件，并上传证书到/opt/sonatype/nexus/etc/ssl目录下\n修改内容为：\n\u0026lt;New id=\u0026quot;sslContextFactory\u0026quot; class=\u0026quot;org.eclipse.jetty.util.ssl.SslContextFactory\u0026quot;\u0026gt; \u0026lt;Set name=\u0026quot;KeyStorePath\u0026quot;\u0026gt;\u0026lt;Property name=\u0026quot;ssl.etc\u0026quot;/\u0026gt;/keystore.jks\u0026lt;/Set\u0026gt; \u0026lt;Set name=\u0026quot;KeyStorePassword\u0026quot;\u0026gt;fsdf!QAZ\u0026lt;/Set\u0026gt; \u0026lt;Set name=\u0026quot;KeyManagerPassword\u0026quot;\u0026gt;fsdf!QAZ\u0026lt;/Set\u0026gt; \u0026lt;Set name=\u0026quot;TrustStorePath\u0026quot;\u0026gt;\u0026lt;Property name=\u0026quot;ssl.etc\u0026quot;/\u0026gt;/keystore.jks\u0026lt;/Set\u0026gt; \u0026lt;Set name=\u0026quot;TrustStorePassword\u0026quot;\u0026gt;1qaz!QAZ\u0026lt;/Set\u0026gt; ...  上传证书到/opt/sonatype/nexus/etc/ssl并重命名为keystore.jks，（证书可以申请免费的证书一般一年的有效期，学习用足够了）\n五、重起服务 $$ docker restart containerId ## containerId 通过docker ps 可以查到 $$\n六、访问测试 地址(domain.com根据自己的实际进行替换) https://domain.com:8082/nexus/\n七、登录测试 以默认用户名admin和密码admin123，及时修改密码，根据业务添加相关用户\n到此nexus安装完成，关地nexus做为maven私服和docker私服，后续会有相关笔记分享，敬请关注！\n  ","permalink":"https://www.easyolap.cn/publications/nexus-001-install-by-docker/","tags":["Nexus","Maven","Docker","Agile","DevOps"],"title":"Nexus install by docker！"},{"categories":null,"contents":" linux如何成功地离线安装docker 系统环境：\nRedhat 7.2 和Centos 7.4实测成功\n近期因项目需要用docker，所以记录一些相关知识，由于生产环境是不能直接连接互联网，尝试在linux中离线安装docker。\n步骤 1.下载https://download.docker.com/linux/static/stable/x86_64/docker-18.09.0.tgz\n2.解压将docker-18.09.0.tgz放置到linux用户目录下面，使用命令：tar xzvf docker-18.09.0.tgz进行解压缩，得到一个文件夹docker，然后使用命令：sudo cp docker/* /usr/bin/将docker文件夹中的内容全部移动到/usr/bin/目录下。\n3.启动守护进程\n使用命令：sudo dockerd \u0026amp;来开启docker守护进程，以此来开启docker的使用  4.验证\n使用命令：docker images、docker ps -a、docker \u0026ndash;version等\n5.停止守护进程\n ps -ef |grep docker 查到进程并kill掉  6.编写系统服务：\n [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity #TasksMax=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target \n7.启动服务\n添加权限 chmod +x /etc/systemd/system/docker.service 重载unit配置文件 systemctl daemon-reload #启动Docker systemctl start docker #设置开机自启 systemctl enable docker.service  8.验证\nsystemctl status docker #查看Docker状态\ndocker -v #查看Docker版本\n","permalink":"https://www.easyolap.cn/publications/docker-001/","tags":["Docker"],"title":"linux如何成功地离线安装docker "},{"categories":null,"contents":" OpenShift Origin 3.9.0手动单机安装\n1.环境\n1.1 硬件 6台 Linux虚拟机： server0, server1, server2, server3 , server4, server5 每台有两块磁盘 ： /dev/vdb, /dev/vdc 每台有一块网卡 ：eth0\n1.2 软件 linux版本： CentOS 7.2.1511 内核版本 ： 3.10.0-327.el7.x86_64 ceph版本： 13.2.2 ceph-deploy版本： 2.0.0\n2.准备工作(所有server)\n2.1 配置静态IP\n10.3.14.0/24\n2.2 生成ssh key\n# ssh-keygen  2.3 配置主机名解析 把如下内容追加到/etc/hosts:\n10.3.14.19 server0 deploy 10.3.14.20 server1 10.3.14.12 server2 10.3.14.13 server3\n修改所有节点的主机名，方法如下： sudo hostnamectl set-hostname server0\n2.4.添加用户 下一件要做的就是，让添加部署的时候要用的用户了。在每个节点都执行：\nsudo useradd -d /data/devops -m devops sudo echo \u0026ldquo;devops ALL = (root) NOPASSWD:ALL\u0026rdquo; | sudo tee /etc/sudoers.d/devops sudo chmod 0440 /etc/sudoers.d/devops sudo passwd devops\n2.5.无密码 ssh 登录 添加部署节点 deploy 对 node 的无密码 ssh 登录，以搭建 Ansible 然后可以批量对 node 执行操作。在 deploy 执行：\nsu devops ssh-keygen cat /data/devops/.ssh/id_rsa.pub  三下回车生成密钥对，然后拷贝 /data/devops/.ssh/id_rsa.pub 文件的内容到各个 node 节点：\nscp -P7777 /data/devops/.ssh/id_rsa.pub devops@server0:/data/devops/.ssh/authorized_keys su test mkdir ~/.ssh vi ~/.ssh/authorized_keys sudo chmod 600 ~/.ssh/authorized_keys  在server0中编辑 ~/.ssh/config 文件：\nHost server0 User devops Port 7777 Host server1 User devops Port 7777 Host server2 User devops Port 7777 Host server3 User devops Port 7777  sudo chmod 600 .ssh/config\n3.安装 Ansible\n以下操作无特殊说明都在 deploy 节点进行操作。\n3.1. 从包管理工具安装 sudo yum -y update \u0026amp;\u0026amp; sudo yum -y install ansible\n3.2. 修改 Ansible 配置文件 在 /etc/ansible/hosts 文件加入：\n[ceph-deploy] localhost ansible_connection=local [ceph-node] server1 server2 server3  3.3. 验证\u0026amp;\u0026amp;测试： ansible all -m ping 结果如下：\n[devops@node-19 .ssh]$ ansible all -m ping server1 | SUCCESS =\u0026gt; { \u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;ping\u0026rdquo;: \u0026ldquo;pong\u0026rdquo; } localhost | SUCCESS =\u0026gt; { \u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;ping\u0026rdquo;: \u0026ldquo;pong\u0026rdquo; } server2 | SUCCESS =\u0026gt; { \u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;ping\u0026rdquo;: \u0026ldquo;pong\u0026rdquo; } server3 | SUCCESS =\u0026gt; { \u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;ping\u0026rdquo;: \u0026ldquo;pong\u0026rdquo; }\n4.Ceph deploy 节点安装\n4.1. 从包管理工具安装 添加国内源 sudo vi /etc/yum.repos.d/ceph.repo\n[Ceph] name=Ceph packages for $basearch baseurl=http://mirrors.163.com/ceph/rpm-mimic/el7/$basearch enabled=1 priority=1 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc\n[Ceph-noarch] name=Ceph noarch packages baseurl=http://mirrors.163.com/ceph/rpm-mimic/el7/noarch enabled=1 priority=1 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc\n[ceph-source] name=Ceph source packages baseurl=http://mirrors.163.com/ceph/rpm-mimic/el7/SRPMS enabled=0 priority=1 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc\n4.2安装：\nsudo yum -y update \u0026amp;\u0026amp; sudo yum -y install ceph-deploy\n5.Ceph 节点安装准备\n5.1. 安装 ntp 修改时区：\nansible all -a \u0026ldquo;sudo timedatectl set-timezone Asia/Shanghai\u0026rdquo; \u0026ndash;sudo\n建议在所有 Ceph 节点上安装 NTP 服务（特别是 Ceph Monitor 节点），并跟同一个 ntp 服务器进行时间同步，以免因时钟漂移导致故障：\nansible all -a \u0026ldquo;sudo yum -y update\u0026rdquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo yum -y install ntp \u0026rdquo; \u0026ndash;sudo\n编辑 ntp 配置文件：\nvi ntp.conf\nrestrict cn.pool.ntp.org server cn.pool.ntp.org\n分发，重启服务：\nansible all -m copy -a \u0026ldquo;src=/etc/ntp.conf dest=/etc/ntp.conf\u0026rdquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo systemctl restart ntpd\u0026rdquo; \u0026ndash;sudo\n5.2. 安装依赖 安装 python：\nansible all -a \u0026ldquo;sudo yum -y install python -y\u0026rdquo; \u0026ndash;sudo 开放端口 ansible all -a \u0026ldquo;sudo firewall-cmd \u0026ndash;permanent \u0026ndash;add-port=6789/tcp\u0026rdquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo firewall-cmd \u0026ndash;permanent \u0026ndash;add-port=6800-7300/tcp\u0026rdquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo firewall-cmd \u0026ndash;reload\u0026rdquo; \u0026ndash;sudo\n3.安装依赖 ansible all -a \u0026ldquo;sudo yum install -y yum-utils \u0026rdquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo yum-config-manager \u0026ndash;add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ \u0026ldquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo yum install \u0026ndash;nogpgcheck -y epel-release \u0026rdquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo rpm \u0026ndash;import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 \u0026rdquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo rm -f /etc/yum.repos.d/dl.fedoraproject.org* \u0026ldquo; \u0026ndash;sudo\nansible all -a \u0026ldquo;sudo yum install redhat-lsb -y \u0026rdquo; \u0026ndash;sudo\n6.ceph-mon 安装\n6.1. 添加 mon 节点 cd /data/devops/ mkdir ceph-cluster \u0026amp;\u0026amp; cd ceph-cluster\nceph-deploy new server1 server2 server3 server4 server5\nceph-deploy new server0 server1 server2\n6.2. 修改配置文件 修改 ceph.conf 文件(vi /data/devops/ceph-cluster/ceph.conf)，添加：\nosd 节点个数 osd_pool_default_size = 3\nosd 节点最小个数 osd_pool_default_min_size = 1\nceph 公共网络 public network = 10.3.14.0/24\n6.3. 安装 ceph 节点 使用国内镜像源安装：\nexport CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/rpm-mimic/el7 export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc\nceph-deploy install server0 server1 server2 server3\n注：如有提示“Delta RPMs disabled because /usr/bin/applydeltarpm not installed”需要在所有节点中安装以下内容： sudo yum provides \u0026lsquo;*/applydeltarpm\u0026rsquo; sudo yum -y install deltarpm\nansible all -a \u0026ldquo;sudo yum provides \u0026lsquo;*/applydeltarpm\u0026rsquo; \u0026ldquo; \u0026ndash;sudo ansible all -a \u0026ldquo;sudo yum -y install deltarpm \u0026rdquo; \u0026ndash;sudo\n6.4. 初始化 mon 节点 ceph-deploy mon create-initial\n如果有重复安装时用ceph-deploy \u0026ndash;overwrite-conf mon create-initial\nceph-deploy admin server1 server2 server3 server4 server5\n7.ceph osd 节点安装\n7.1. 查看集群 uuid 集群的 uuid 就是这个 ceph 集群的唯一标识，后面要用：\ncat /etc/ceph/ceph.conf [global] fsid = 9d21dbff-dcf3-41d0-88a9-10bbdefde284 mon_initial_members = server0, server1, server2 mon_host = 10.3.14.19,10.3.14.20,10.3.14.12 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx\nosd 节点个数 osd_pool_default_size = 3\nosd 节点最小个数 osd_pool_default_min_size = 1\nceph 公共网络 public network = 10.3.14.0/24\n其中 9d21dbff-dcf3-41d0-88a9-10bbdefde284 就是 uuid\nansible all -a \u0026ldquo;sudo chmod +r /etc/ceph/ceph.client.admin.keyring \u0026rdquo; \u0026ndash;sudo\n7.2. 安装 osd ssh 到各个节点，执行以下命令：\nmkdir /data/devops/osd0 sudo chown ceph: /data/devops/osd0/ sudo ceph-disk prepare \u0026ndash;cluster ceph \u0026ndash;cluster-uuid 9d21dbff-dcf3-41d0-88a9-10bbdefde284 \u0026ndash;fs-type ext4 /data/devops/osd0/ sudo ceph-disk activate /data/devops/osd0/\n以上命令是在 server1 上执行的，请将 uuid 替换成自己的，–fs-type 是 ext4，请换成自己的类型。然后将 osd0 替换掉对应节点的 osd 编号。\n7.3. 查看集群健康状况 当所有节点都安装完成，可以在任一节点执行以下命令查看集群健康状况:\nceph -s\n或者查看集群健康状况 $ ceph health HEALTH_OK\n查看集群 OSD 信息 $ ceph osd tree\n8.清理 Ceph 安装包:\nceph-deploy purge server1 server2 server3 server4 server5 清理配置:\nceph-deploy purgedata server0 server1 server2 server3 server4 server5 ceph-deploy forgetkeys\n9.问题\n1：ERROR: missing keyring, cannot use cephx for authentication 解决：1.确认/etc/ceph/ceph.client.admin.keyring 是否存在，如不存在请确认hostname.修改主机名后需重新安装cehp. 2.设置权限 sudo chmod +r /etc/ceph/ceph.client.admin.keyring\n","permalink":"https://www.easyolap.cn/publications/ceph-cluster/","tags":["DevOps","Cluster FS","Continuous Integration","Continuous Delivery","CI/CD pipelines","docker","agile","Culture"],"title":"使用ceph-deploy 2.0.0 部署ceph "},{"categories":null,"contents":"使用kubeadm安装k8s all-in-one单机测试环境\n1.环境准备\n 使用Oracle VM VirtualBox安装虚拟机\n 配置双网卡，网卡1使用net方式;网卡2使用host-only（配置静态ip 192.168.134.0网段）;这样就可以实现虚拟机与宿主机，宿主机与虚拟机之前的通信。\n 安装centos 7操作系统（CentOS-7-x86_64-DVD-1708.iso），最小安装版。\n 配置国内yum源，我采用的是阿里的镜像。\n 安装常用工具\n yum install -y telnet net-tools vim wget curl lrssz git unzip  关闭防火墙\n systemctl stop firewalld \u0026amp;\u0026amp; systemctl disable firewalld  关闭selinux\nsetenforce 0 sed -i s/\u0026quot;SELINUX=enforcing\u0026quot;/\u0026quot;SELINUX=disabled\u0026quot;/g /etc/selinux/config sed -i s/\u0026quot;^SELINUXTYPE=targeted\u0026quot;/\u0026quot;\u0026quot;/g /etc/selinux/config  安装docker-ce curl -fsSL https://get.docker.com/ | sh\n 重启docker，配置自起 systemctl enable docker \u0026amp;\u0026amp; systemctl start docker\n 配置系统参数\ncat \u0026lt;\u0026lt;EOF | tee /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl -p /etc/sysctl.d/k8s.conf  关闭swap swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 vim /etc/fstab 修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭\n 安装k8s前重起一次操作，以验证以上配置是否完全生效。(可选) reboot\n  2.安装kubelet，kubectl，kubeadm，kubernetes-cni\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 EOF yum -y install epel-release yum clean all yum makecache yum -y install kubelet kubeadm kubectl kubernetes-cni systemctl enable docker \u0026amp;\u0026amp; systemctl start docker systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet  3.上传镜像并导入到本地镜像库中。 由于网络原因访问不到k8s.gcr.io，采用先载然后导入到本地镜像库。\n4.kubeadm安装k8s\nkubeadm init --kubernetes-version=v1.11.0 --pod-network-cidr=10.244.0.0/16  滚动日志，直到有以下类似内容出现：\n Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.134.3:6443 --token hro1kc.upqh2abab12pfd0o --discovery-token-ca-cert-hash sha256:3f843b4ccb23c0e0d54b5e454d2404323e863c7e065aa20e042d386b31a271be  5.复制配置信息到root用户下\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  6.master节点负载node\nkubectl taint nodes --all node-role.kubernetes.io/master-  7.安装 flannel\nmkdir -p /etc/cni/net.d/ cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/cni/net.d/10-flannel.conf { “name”: “cbr0”, “type”: “flannel”, “delegate”: { “isDefaultGateway”: true } } EOF mkdir /usr/share/oci-umount/oci-umount.d -p mkdir /run/flannel/ cat \u0026lt;\u0026lt;EOF\u0026gt; /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.1.0/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true EOF kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml  8.安装dashboard 直接在线安装 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.0/src/deploy/recommended/kubernetes-dashboard.yaml 先下载后安装 到https://github.com/kubernetes/dashboard/releases下载指定版本的kubernetes-dashboard.yaml文件，上传到服务器 kubectl apply -f /home/beyond/kubernetes-dashboard.yaml\n9.查看运行端口\nkubectl describe --namespace kube-system service kubernetes-dashboard  10.获取令牌\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')  11.安装heapster\ngit clone https://github.com/dolphintwo/k8s-manual-files.git cd k8s-manual-files/addons kubectl create -f kube-heapster/influxdb/ kubectl create -f kube-heapster/rbac/ kubectl get pods --all-namespaces  重启dashboard即可见 访问地址： https://192.168.134.3:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n12.常用的命令\n查看指定pod详情\nkubectl describe pod -n=kube-system pod_name  查看某命名空间下的pods，-n指定的命名空间名 kubectl get pods -n=kube-system\n查看日志\nkubectl logs -n=kube-system -f --tail=10 pod_name  ","permalink":"https://www.easyolap.cn/publications/kubernetes-all-in-one/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","docker","agile","Culture"],"title":"使用kubeadm安装k8s all-in-one单机测试环境 (版本1.11.1)"},{"categories":null,"contents":" 本文转载自：DevOps小站 官方网站，原文地址：https://www.easyolap.cn/publications/golang-001\n 数组、切片和映射小结\n 数组是构造切片和映射的基石。\n Go语言里切片经常用来处理数据的集合，映射用来处理具有键值对结构的数据。\n 内置函数make 可以创片和映射，并指定原始的长度和容量。也可以直接使切片和映射字央量，或者使用字面量作为变量作为变量的初始值。\n 切片有容量限制，不过可以使用内置的append函数扩展容量。\n 映射的增长设有容量或者任何限制。\n 内置函数len可以用来获取切片或映射的长度。\n 内置函数cap只能用于切片。\n 通过组合，可以创建多维数组和多维切片。也可以使用切片或者其他映射作为映射 的值。但是切片不能用作映射的键。\n 将切片或者映射传递给函数成本很小，并且不会复制底层的数据结构。\n  ","permalink":"https://www.easyolap.cn/publications/golang-001/","tags":["golang","数组","切片","映射"],"title":"golang学习笔记-关于数组，切片，映射"},{"categories":null,"contents":"OpenShift Origin 3.9.0手动单机安装\n1.配置主机 修改主机名为openshift.demo.com hostnamectl set-hostname openshift.demo.com\n开启SELINUX 修改/etc/selinux/config SELINUX=enforcing SELINUXTYPE=targeted 激活网络 \\# nmcli con show  docker0 1a211fa6-1001-4fa9-b5c8-e3b2dcf73e5a bridge docker0 ens192 f16e6b7a-e593-4722-9ae4-1bdfa1fa4b4a ethernet ens192\n# nmcli con up ens192 # nmcli con mod ens192 connection.autoconnect yes # systemctl restart NetworkManager\n2.安装docker 安装依赖 yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion 安装docker yum install -y docker 配置Docker镜像服务器\n中国科技大学的镜像服务器进行加速。修改/etc/sysconfig/docker文件，在OPTIONS变量中追加\u0026ndash;registry-mirror=https://docker.mirrors.ustc.edu.cn \u0026ndash;insecure-registry=172.30.0.0/16。\n3.下载openshift-origin-server-v3.9.0-191fece-linux-64bit.tar.gz\n4.解压openshift-origin-server-v3.9.0-191fece-linux-64bit.tar.gz到 /opt/openshift\n5.添加到PATH\n6.测试docker是否能正常下载镜像 docker pull busybox\n7.执行启动命令,开始下载指定版本v3.9.0所需的镜像文件 oc cluster up \u0026ndash;version=v3.9.0 \u0026ndash;public-hostname=openshift.demo.com\n 会有类似以下信息： Pulling image openshift/origin:v3.9.0 Pulled 1/4 layers, 26% complete  。。。\nPulled 4\u0026frasl;4 layers, 100% complete\n表示下载完成了并启动服务\n8.启动完成后访问系统 https://openshift.demo.com:8443\n9.\n","permalink":"https://www.easyolap.cn/publications/openshift-001/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","docker","agile","Culture"],"title":"OpenShift Origin  3.9.0手动单机安装"},{"categories":null,"contents":"[OSEv3:children] masters nodes etcd nfs\n[OSEv3:vars] ansible_ssh_user=devops openshift_deployment_type=origin #因采用虚拟机部署学习 配置此选项跳过主机硬件信息检查 openshift_disable_check=disk_availability,docker_storage,memory_availability,docker_image_availability openshift_master_identity_providers=[{\u0026lsquo;name\u0026rsquo;:\u0026lsquo;htpasswd_auth\u0026rsquo;,\u0026lsquo;login\u0026rsquo;:\u0026lsquo;true\u0026rsquo;,\u0026lsquo;challenge\u0026rsquo;:\u0026lsquo;true\u0026rsquo;,\u0026lsquo;kind\u0026rsquo;:\u0026lsquo;HTPasswdPasswordIdentityProvider\u0026rsquo;,}]\nopenshift_master_default_subdomain=okd.easyolap.cn openshift_deployment_type=origin os_firewall_use_firewalld=true\n[masters] server0\n[etcd] server0\n[nodes] server1 openshift_node_group_name=\u0026lsquo;node-config-master\u0026rsquo; server2 openshift_node_group_name=\u0026lsquo;node-config-compute\u0026rsquo; server3 openshift_node_group_name=\u0026lsquo;node-config-compute\u0026rsquo;\n[nfs] server0\n安装docker sudo curl -fsSL https://get.docker.com/ | sh\n问题 ：fatal: [server0]: UNREACHABLE! =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;Failed to connect to the host via ssh: ssh: connect to host server0 port 22: Connection refused\\r\\n\u0026rdquo;, \u0026ldquo;unreachable\u0026rdquo;: true} 解决 sudo ansible-playbook openshift-ansible/playbooks/prerequisites.yml \u0026ndash;limit @/data/devops/openshift-ansible/playbooks/prerequisites.retry\n","permalink":"https://www.easyolap.cn/publications/openshift-002/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","docker","agile","Culture"],"title":"OpenShift Origin  3.9.0手动单机安装"},{"categories":null,"contents":" Docker运行mysql镜像 启动mysql镜像\n[root@CentOS ~]# docker run -d -e MYSQL_ROOT_PASSWORD=admin \u0026ndash;name mysql -v /data/mysql/etc:/etc/mysql/conf.d -v /data/mysql/data:/var/lib/mysql -v /etc/localtime:/etc/localtime:ro mysql\n说明：\n1.把数据文件存贮在宿主机中的/data/mysql/data目录下，所以挂载/data/mysql/data到/var/lib/mysql\n2.采用宿主机中的配置启动mysql服务，所以挂载 /data/mysql/etc到/etc/mysql/conf.d 配置文件见下文的my.cnf文件\n3.使容器与宿主机时间同步，挂载/etc/localtime到/etc/localtime:ro 只读的方式。\nmy.cnf\n[mysqld]\nuser = mysql\ndefault-storage-engine = InnoDB\nsocket = /var/lib/mysql/mysql.sock\npid-file = /var/lib/mysql/mysql.pid\nskip-external-locking\nskip-name-resolve\n\\# MyISAM \\#\nkey-buffer-size = 32M\n# SAFETY #\nmax-allowed-packet = 16M\nmax-connect-errors = 1000000\n# DATA STORAGE #\ndatadir = /var/lib/mysql\n# CACHES AND LIMITS #\ntmp-table-size = 32M\nmax-heap-table-size = 32M\nquery-cache-type = 0\nquery-cache-size = 0\nmax-connections = 500\nthread-cache-size = 50\nopen-files-limit = 65535\ntable-definition-cache = 4096\ntable-open-cache = 4096\n# INNODB #\ninnodb-flush-method = O_DIRECT\ninnodb-log-files-in-group = 2\ninnodb-log-file-size = 64M\ninnodb-flush-log-at-trx-commit = 1\ninnodb-file-per-table = 1\ninnodb-buffer-pool-size = 592M\n# LOGGING #\nlog-error = /var/lib/mysql/mysql-error.log\nlog-queries-not-using-indexes = 1\nslow-query-log = 1\nslow-query-log-file = /var/lib/mysql/mysql-slow.log\n","permalink":"https://www.easyolap.cn/publications/mysql-by-docker/","tags":["mysql","docker","Agile","DevOps"],"title":"Mysql install by Docker！"},{"categories":null,"contents":" 本文转载自：DevOps小站 官方网站，原文地址：https://www.easyolap.cn/publications/mysql-by-docker/\n 一、编译安装MySQL前准备工作\n安装编译源码所需的工具和库\nyum -y install gcc gcc-c++ ncurses-devel perl openssl-devel bison\n安装cmake（记得好像从mysql 5.5开始需要cmake编译安装），可从https://cmake.org/download/ 中下载。\ntar zxvf cmake-3.6.1.tar.gz\ncd cmake-3.6.1\n./bootstrap\nmake \u0026amp;\u0026amp; make install\n二、创建用户及MySQL所需目录\n新增mysql用户\ngroupadd -r mysql\nuseradd -r -g mysql mysql\n新建MySQL所需目录\nmkdir -p /data/mysql/server\nmkdir -p /data/mysql/data\nmkdir -p /data/mysql/etc\n三、编译安装MySQL\n可从http://dev.mysql.com/downloads/mysql/ 下载mysql源码(MySQL Community Server 5.6.32 )。\ntar zxvf mysql-5.6.32.tar.gz\ncd mysql-5.6.32\n\u0026ldquo;cmake -DCMAKE_INSTALL_PREFIX=/data/mysql/server\n-DDEFAULT_CHARSET=utf8\\ -DDEFAULT_COLLATION=utf8_general_ci\\ -DWITH_INNOBASE_STORAGE_ENGINE=1\\ -DWITH_ARCHIVE_STORAGE_ENGINE=1\\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1\\ -DMYSQL_DATADIR=/data/mysql/data\\ -DMYSQL_TCP_PORT=3306\\ -DENABLE_DOWNLOADS=1\\ -DSYSCONFDIR=/data/mysql/etc\\ -DWITH_SSL=system\\ -DWITH_ZLIB=system\\ -DWITH_LIBWRAP=0 \u0026ldquo;\nmake \u0026amp;\u0026amp; make install\nDCMAKE_INSTALL_PREFIX=dir_name 设置mysql安装目录\n修改mysql目录权限\ncd /data/mysql/server\nchown -R mysql:mysql ./\ncd /data/mysql/data\nchown -R mysql:mysql ./\n初始化mysql数据库\ncd /data/mysql/server/\n./scripts/mysql_install_db \u0026ndash;user=mysql \u0026ndash;datadir=/data/mysql/data\n编辑MySQL配置文件\nmv /etc/my.cnf /data/mysql/etc/my.cnf\nchown -R mysql:mysql /data/mysql/etc/my.cnf\n编辑my.cnf，my.cnf可在percona官网中及按照自己的情况生成。网址如下：https://tools.percona.com/wizard 。\n[mysql]\n# CLIENT #\nport = 3306\nsocket = /data/mysql/data/mysql.sock\n[mysqld]\n# GENERAL #\nuser = mysql\ndefault-storage-engine = InnoDB\nsocket = /data/mysql/data/mysql.sock\npid-file = /data/mysql/data/mysql.pid\nskip-external-locking\nskip-name-resolve\n# MyISAM #\nkey-buffer-size = 32M\nmyisam-recover = FORCE,BACKUP\n# SAFETY #\nmax-allowed-packet = 16M\nmax-connect-errors = 1000000\n# DATA STORAGE #\ndatadir = /data/mysql/data\n# BINARY LOGGING #\nlog-bin = /data/mysql/data/mysql-bin\nexpire-logs-days = 14\nsync-binlog = 1\n# REPLICATION #\nskip-slave-start = 1\nrelay-log = /data/mysql/data/relay-bin\nslave-net-timeout = 60\n# CACHES AND LIMITS #\ntmp-table-size = 32M\nmax-heap-table-size = 32M\nquery-cache-type = 0\nquery-cache-size = 0\nmax-connections = 500\nthread-cache-size = 50\nopen-files-limit = 65535\ntable-definition-cache = 4096\ntable-open-cache = 4096\n# INNODB #\ninnodb-flush-method = O_DIRECT\ninnodb-log-files-in-group = 2\ninnodb-log-file-size = 64M\ninnodb-flush-log-at-trx-commit = 1\ninnodb-file-per-table = 1\ninnodb-buffer-pool-size = 592M\n# LOGGING #\nlog-error = /data/mysql/data/mysql-error.log\nlog-queries-not-using-indexes = 1\nslow-query-log = 1\nslow-query-log-file = /data/mysql/data/mysql-slow.log\n复制MySQL启动文件及其命令加入PATH\ncp support-files/mysql.server /etc/init.d/mysqld\nvim /etc/profile.d/mysql.sh\nPATH=/data/mysql/server/bin:/data/mysql/server/lib:$PATH\nexport PATH\nsource /etc/profile.d/mysql.sh\n启动MySQL并增加启动项\nservice mysqld start\nchkconfig mysqld on\n设置MySQL登录权限\ndrop user \u0026ldquo;@localhost;\ndrop user \u0026ldquo;@hostname;\nupdate mysql.user set password=password(\u0026lsquo;3qw0ku7\u0026rsquo;);\nflush privileges;\n至此，MySQL编译安装完成。\n","permalink":"https://www.easyolap.cn/publications/mysql-src-install/","tags":["mysql","docker","agile"],"title":"Mysql源码安装！"},{"categories":null,"contents":"整理中，敬请期待\u0026hellip;\n","permalink":"https://www.easyolap.cn/projects/creations/devops-wiki/","tags":["DevOps","Agile","Java","Microservice","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"记录DevOps实施与推广相关知识"},{"categories":null,"contents":" 大家好，我叫 Frank Li，是一名程序员。 我的经历  1999-2002 沈阳工程学院信息工程系 2007-2012 辽宁广播电视大学(本科，工商管理专业) 2002-2005 测试开发（东软）。 2005-2006 JAVA程序员（千像公司，互联网公司） 2006-2007 程序员（新思软件，软件外包） 2007-2008 架构师（盛生医药，合伙创业电子商务） 2009-2009 架构师（智诚祥科技，政府项目） 2010-2018 资深软件工程师，技术负责人（东软） 2018-至今 资深DevOps工程师（新炬网络）  我的技能  编程语言：JAVA,了解PHP,Python，捣鼓 Go…… 熟练使用Ofbiz,spring boot等流行框架,熟悉RESTful,webservice,microservice等web服务架构; 熟练使用Maven对项目进行模块化管理和组织; 掌握Nexus server搭建，熟练构建私有maven仓库和Docker镜像库; 掌握jenkins持续集成工具与流程,了解DevOps思想和工具,在项目应用让生产端变得敏捷; 熟悉JIRA搭建，并用其进行管理项目; 熟悉Confluence对知识的管理，积累经验提高团队的战斗力; 掌握Linux下makefile;了解Android开发; 能够熟练操作和使用Mysql、Oracle等数据库;熟练操作cassandra,hbase等nosql数据库 掌握docker虚拟化技术,撑握Dockerfile以及k8s模板编写; 成功实施过kubernetes 1.9.1; 熟练使用Linux操作系统,进行运营环境的部署; 熟悉Tomcat等服务器的使用和部署; 熟悉Selenium、dbunit和Junit/testng进行自动化/半自动化测试; 了解apache spark/beam大数据处理工具; 熟悉软件开发的整个周期;能够使用和指导使用分布式版本控制工具进行多地团队合作开发;  生活中的我  喜欢思考，对未知世界保持好奇心。 有点宅。  一些链接  JAVAEYE博客：surpass-li.iteye.com Github: github.com/surpass Email：surpass_li@aliyun.com(surpass.li@gmail.com) 微信号：s46488820\n  声明 本博客所有文章皆为原创。转载请注明原文链接，并最好与本人联系。谢谢！\n","permalink":"https://www.easyolap.cn/about/","tags":null,"title":"关于我"},{"categories":null,"contents":" This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026quot;HTML\u0026quot;, \u0026quot;JSON\u0026quot;]  Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026quot;contents\u0026quot;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026quot;tags\u0026quot;:{{ .Params.tags | jsonify }}{{end}}, \u0026quot;categories\u0026quot; : {{ .Params.categories | jsonify }}, ...  Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026quot;title\u0026quot;, \u0026quot;contents\u0026quot;, \u0026quot;tags\u0026quot;, \u0026quot;categories\u0026quot; ]  ","permalink":"https://www.easyolap.cn/search/","tags":null,"title":"Search Results"}]